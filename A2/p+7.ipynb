{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53e77678"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Load the GPT-2 tokenizer and the GPT-2 language model (`GPT2LMHeadModel`) from the `transformers` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52a1cd07"
      },
      "source": [
        "**Reasoning**:\n",
        "First, it's necessary to install the `transformers` library as it is not part of the standard Python distribution and is required for loading the GPT-2 tokenizer and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd59cab",
        "outputId": "850570ff-e64f-41aa-e34e-e7c5497dba8a"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d732573"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `transformers` library is installed, the next logical step is to import the required classes and then load the pre-trained GPT-2 tokenizer and model as specified in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ac2cb4",
        "outputId": "65c985c2-118f-4fb3-9d7d-b77fbafe3c77"
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the pre-trained GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Load the pre-trained GPT-2 language model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "print(\"GPT-2 tokenizer and model loaded successfully.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 tokenizer and model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09587d42"
      },
      "source": [
        "**Reasoning**:\n",
        "The `poem_text` needs to be processed line by line. The next step is to split the poem into individual lines based on the newline character, as indicated by the task description.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cce3de70",
        "outputId": "2253fb78-1bb0-41ea-c454-4e1ff9f69f2e"
      },
      "source": [
        "poem_lines = poem_text.strip().split('\\n')\n",
        "print(f\"Number of lines in the poem: {len(poem_lines)}\")\n",
        "print(\"First 3 lines:\")\n",
        "for i, line in enumerate(poem_lines[:3]):\n",
        "    print(f\"Line {i+1}: {line}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in the poem: 15\n",
            "First 3 lines:\n",
            "Line 1: One must have a mind of winter\n",
            "Line 2: To regard the frost and the boughs\n",
            "Line 3: Of the pine-trees crusted with snow;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccabe574"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the poem is split into lines, the next step is to iterate through each line, identify its last word, and prepare the preceding text as input for the GPT-2 model to predict the next word. We'll also initialize an empty list to store the reconstructed lines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9a15d63",
        "outputId": "be9e0ea5-3eef-44f3-d792-1e0a7d48c98a"
      },
      "source": [
        "reconstructed_lines = []\n",
        "\n",
        "print(\"Processing poem lines...\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing poem lines...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49dd524a",
        "outputId": "3e1a3538-b35f-4046-cce1-85f0fd1071c6"
      },
      "source": [
        "poem_text = \"\"\"One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\"\"\"\n",
        "\n",
        "print(\"Poem text updated with the new content.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poem text updated with the new content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1db91bb4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `poem_text` variable has been updated, the next step is to split the new poem into individual lines based on the newline character, as indicated by the task description."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5d6cba3",
        "outputId": "b3322b05-8cc4-4d03-e5e9-45298298a24d"
      },
      "source": [
        "poem_lines = poem_text.strip().split('\\n')\n",
        "print(f\"Number of lines in the poem: {len(poem_lines)}\")\n",
        "print(\"First 3 lines:\")\n",
        "for i, line in enumerate(poem_lines[:3]):\n",
        "    print(f\"Line {i+1}: {line}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in the poem: 15\n",
            "First 3 lines:\n",
            "Line 1: One must have a mind of winter\n",
            "Line 2: To regard the frost and the boughs\n",
            "Line 3: Of the pine-trees crusted with snow;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "187c9b04"
      },
      "source": [
        "**Reasoning**:\n",
        "Before processing the lines of the new poem, the `reconstructed_lines` list needs to be re-initialized to ensure it's empty for the new processing run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d337cbb7",
        "outputId": "0124fbf5-d6ad-481a-d83e-099e26ad0782"
      },
      "source": [
        "reconstructed_lines = []\n",
        "\n",
        "print(\"Re-initialized reconstructed_lines list and proceeding with poem processing...\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-initialized reconstructed_lines list and proceeding with poem processing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cecb3cf"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, iterate through each `poem_lines` to process each line of the new poem. For each line, identify the last word and the prefix that will be used as input for the GPT-2 model. The code also includes the previously added logic for cleaning special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0aa0d55",
        "outputId": "46732bb1-6089-413f-abb5-2a93f87acc8b"
      },
      "source": [
        "import torch\n",
        "import re\n",
        "\n",
        "for i, line in enumerate(poem_lines):\n",
        "    # Split line into words and clean each word from special characters\n",
        "    cleaned_line_words = [re.sub(r'[^\\w]', '', word) for word in line.split()]\n",
        "    # Filter out empty strings that might result from cleaning (e.g., '...')\n",
        "    words = [word for word in cleaned_line_words if word]\n",
        "\n",
        "    if not words:\n",
        "        reconstructed_lines.append('')\n",
        "        continue\n",
        "\n",
        "    last_word = words[-1]\n",
        "    prefix = ' '.join(words[:-1])\n",
        "\n",
        "    # Encode the prefix\n",
        "    input_ids = tokenizer.encode(prefix, return_tensors='pt')\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = outputs.logits\n",
        "\n",
        "    # Get the probabilities for the next token\n",
        "    next_token_logits = predictions[0, -1, :]\n",
        "    # Sort by probability in descending order and get token IDs\n",
        "    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "\n",
        "    # Get the 7th most likely token (index 6, since 0-indexed)\n",
        "    if len(sorted_indices) > 6:\n",
        "        predicted_token_id = sorted_indices[6].item()\n",
        "        predicted_word = tokenizer.decode([predicted_token_id]).strip()\n",
        "    else:\n",
        "        # Fallback if fewer than 7 predictions are available\n",
        "        predicted_word = \"<UNKNOWN>\"\n",
        "        if len(sorted_indices) > 0:\n",
        "            predicted_token_id = sorted_indices[0].item()\n",
        "            predicted_word = tokenizer.decode([predicted_token_id]).strip()\n",
        "\n",
        "    # Reconstruct the line with the new word\n",
        "    reconstructed_line = f\"{prefix} {predicted_word}\".strip()\n",
        "    reconstructed_lines.append(reconstructed_line)\n",
        "\n",
        "print(\"Finished processing all lines with the new poem.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing all lines with the new poem.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24088d9c"
      },
      "source": [
        "**Reasoning**:\n",
        "The final step is to display the reconstructed poem with the replaced last words. The `reconstructed_lines` list now holds all the processed lines from the new poem, so joining them into a single string and printing the result will fulfill the task requirement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4529fdba",
        "outputId": "6dbef6ba-b14c-4e59-cbdc-b2241fa867c1"
      },
      "source": [
        "print(\"Reconstructed Poem with 7th Most Likely Words (New Poem):\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print('\\n'.join(reconstructed_lines))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed Poem with 7th Most Likely Words (New Poem):\n",
            "--------------------------------------------------\n",
            "One must have a mind of her\n",
            "To regard the frost and the death\n",
            "Of the pinetrees crusted with salt\n",
            "And have been cold a long way\n",
            "To behold the junipers shagged with white\n",
            "The spruces rough in the distant horizon\n",
            "Of the January sun and not to speak\n",
            "Of any misery in the sound of the sound\n",
            "In the sound of a few shots\n",
            "Which is the sound of the voice\n",
            "Full of the same day\n",
            "That is blowing in the same bare air\n",
            "For the listener who listens in the second\n",
            "And nothing himself has\n",
            "Nothing that is not there and the nothing that isn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fe5ae41"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, iterate through each `poem_lines` to process each line. For each line, identify the last word and the prefix that will be used as input for the GPT-2 model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50926421",
        "outputId": "ad8a7c6a-b7e3-4d6e-f31b-7acd9b58c223"
      },
      "source": [
        "import torch\n",
        "import re # Import re module\n",
        "\n",
        "for i, line in enumerate(poem_lines):\n",
        "    # Split line into words and clean each word from special characters\n",
        "    cleaned_line_words = [re.sub(r'[^\\w]', '', word) for word in line.split()]\n",
        "    # Filter out empty strings that might result from cleaning (e.g., '...')\n",
        "    words = [word for word in cleaned_line_words if word]\n",
        "\n",
        "    if not words:\n",
        "        reconstructed_lines.append('')\n",
        "        continue\n",
        "\n",
        "    last_word = words[-1]\n",
        "    prefix = ' '.join(words[:-1])\n",
        "\n",
        "    # Encode the prefix\n",
        "    input_ids = tokenizer.encode(prefix, return_tensors='pt')\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = outputs.logits\n",
        "\n",
        "    # Get the probabilities for the next token\n",
        "    next_token_logits = predictions[0, -1, :]\n",
        "    # Sort by probability in descending order and get token IDs\n",
        "    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "\n",
        "    # Get the 7th most likely token (index 6, since 0-indexed)\n",
        "    if len(sorted_indices) > 6:\n",
        "        predicted_token_id = sorted_indices[6].item()\n",
        "        predicted_word = tokenizer.decode([predicted_token_id]).strip()\n",
        "    else:\n",
        "        # Fallback if fewer than 7 predictions are available\n",
        "        predicted_word = \"<UNKNOWN>\"\n",
        "        if len(sorted_indices) > 0:\n",
        "            predicted_token_id = sorted_indices[0].item()\n",
        "            predicted_word = tokenizer.decode([predicted_token_id]).strip()\n",
        "\n",
        "    # Reconstruct the line with the new word\n",
        "    reconstructed_line = f\"{prefix} {predicted_word}\".strip()\n",
        "    reconstructed_lines.append(reconstructed_line)\n",
        "\n",
        "print(\"Finished processing all lines.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing all lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7ecb07"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires displaying the reconstructed poem with the replaced last words. The `reconstructed_lines` list now holds all the processed lines, so the next step is to join them into a single string, separated by newline characters, and then print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6625521",
        "outputId": "6a39b022-3f44-48ef-b325-986b7024bc88"
      },
      "source": [
        "print(\"Reconstructed Poem with 7th Most Likely Words:\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print('\\n'.join(reconstructed_lines))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed Poem with 7th Most Likely Words:\n",
            "--------------------------------------------------\n",
            "One must have a mind of her\n",
            "To regard the frost and the death\n",
            "Of the pinetrees crusted with salt\n",
            "And have been cold a long way\n",
            "To behold the junipers shagged with white\n",
            "The spruces rough in the distant horizon\n",
            "Of the January sun and not to speak\n",
            "Of any misery in the sound of the sound\n",
            "In the sound of a few shots\n",
            "Which is the sound of the voice\n",
            "Full of the same day\n",
            "That is blowing in the same bare air\n",
            "For the listener who listens in the second\n",
            "And nothing himself has\n",
            "Nothing that is not there and the nothing that isn\n",
            "One must have a mind of her\n",
            "To regard the frost and the death\n",
            "Of the pinetrees crusted with salt\n",
            "And have been cold a long way\n",
            "To behold the junipers shagged with white\n",
            "The spruces rough in the distant horizon\n",
            "Of the January sun and not to speak\n",
            "Of any misery in the sound of the sound\n",
            "In the sound of a few shots\n",
            "Which is the sound of the voice\n",
            "Full of the same day\n",
            "That is blowing in the same bare air\n",
            "For the listener who listens in the second\n",
            "And nothing himself has\n",
            "Nothing that is not there and the nothing that isn\n"
          ]
        }
      ]
    }
  ]
}